import torch as t
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import mobilenet_v2
from hypnettorch.mnets.mnet_interface import MainNetInterface

class Decoder(nn.Module):
    def __init__(self, has_bias):
        super(Decoder, self).__init__()
        self._has_bias = has_bias
        # self.last_weight = None
        # self.last_weight_int = None

        self.bn7_3 = nn.BatchNorm2d(num_features=512)
        self.bn8_1 = nn.BatchNorm2d(num_features=256)
        self.bn8_2 = nn.BatchNorm2d(num_features=256)
        self.bn9_1 = nn.BatchNorm2d(num_features=128)
        self.bn9_2 = nn.BatchNorm2d(num_features=128)
        self.bn10_1 = nn.BatchNorm2d(num_features=64)
        self.bn10_2 = nn.BatchNorm2d(num_features=64)
    
        self.conv8_1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)
        self.conv8_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.conv9_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.conv10_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv10_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

        # layers not actually used in forward but providing the param shapes
        self._conv7_3 = nn.Conv2d(1280, 512, kernel_size=3, padding=1)
        self._conv9_1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)
        self._output = nn.Conv2d(64, 1, kernel_size=1, padding=0)
        self._external_params = []
        self._external_params.extend(self._conv7_3.parameters())
        self._external_params.extend(self._conv9_1.parameters())
        self._external_params.extend(self._output.parameters())

        
    def forward(self, xb, weights):
        w_weights = []
        b_weights = []
        for i, p in enumerate(weights):
            if i % 2 == 1:
                b_weights.append(p)
            else:
                w_weights.append(p)

        # TEMP: just to make sure the hypernetwork is learning
        # with t.no_grad():
        #     if self.last_weight is not None:
        #         print(f"Hypernetwork {t.norm(self.last_weight - w_weights[0])}")
            
        #     if self.last_weight_int is not None:
        #         print(f"Main Network {t.norm(self.last_weight_int - self.bn10_2.weight)}")
            
        #     self.last_weight = w_weights[0]
        #     self.last_weight_int = self.bn10_2.weight.clone()
        

        xb = F.relu(F.conv2d(xb, w_weights[0], b_weights[0], padding=1))
        xb = F.interpolate(xb, scale_factor=2, mode="bilinear", align_corners=False)

        xb = F.relu(self.bn8_1(self.conv8_1(xb)))
        xb = F.interpolate(xb, scale_factor=2, mode="bilinear", align_corners=False)
        xb = F.relu(self.bn8_2(self.conv8_2(xb)))
        xb = F.interpolate(xb, scale_factor=2, mode="bilinear", align_corners=False)

        xb = F.relu(F.conv2d(xb, w_weights[1], b_weights[1], padding=1))
        xb = F.interpolate(xb, scale_factor=2, mode="bilinear", align_corners=False)
        xb = F.relu(self.bn9_2(self.conv9_2(xb)))
        xb = F.interpolate(xb, scale_factor=2, mode="bilinear", align_corners=False)

        xb = F.relu(self.bn10_1(self.conv10_1(xb)))
        xb = F.relu(self.bn10_2(self.conv10_2(xb)))

        xb = F.conv2d(xb, w_weights[2], b_weights[2], padding=0)
        return xb

    # gets the shape of the parameters of which we expect the weights to be generated by the hypernetwork
    def get_external_params(self):
        return self._external_params


class Student(nn.Module, MainNetInterface):
    def __init__(self):
        nn.Module.__init__(self)
        MainNetInterface.__init__(self)

        # TODO: can we skip this???
        self._has_fc_out = False 
        self._mask_fc_out = False
        self._has_linear_out = False
        self._has_bias = True

        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()

        self.activation = {}
        self.encoder = self.mobilenetv2_pretrain()
        self.decoder = self.simple_decoder()
        self.sigmoid = nn.Sigmoid()

        # params that will be trained by the hypernetwork
        self._external_params = []
        self._external_params.extend(self.decoder.get_external_params())
        self._external_param_shapes = []
        for param in self._external_params:
            self._external_param_shapes.append(list(param.size()))

        # params that will be trained by the model itself
        self._internal_params = [p for p in self.parameters() if p not in set(self._external_params)]

        self._param_shapes = []
        for param in self._internal_params:
            self._param_shapes.append(list(param.size()))


        self._is_properly_setup()

    def forward(self, xb, weights):
        enc = self.encoder(xb)
        dec = self.decoder(enc, weights)
        prob = self.sigmoid(dec)
        return prob


    def freeze_encoder(self):
        for param in self.encoder.parameters():
            param.requires_grad_(False)

    def unfreeze_encoder(self):
        for param in self.encoder.parameters():
            param.requires_grad_(True)
    
    def external_param_shapes(self):
        return self._external_param_shapes

    ####################
    # MainNetInterface #
    ####################

    def distillation_targets(self):
        return None # we do not have any distillation targets

    #########
    # Other #
    #########

    def mobilenetv2_pretrain(self, pretrained=True, forward_hook_index=range(1,19,1)):
        model = mobilenet_v2(pretrained=pretrained, progress=False)
        features = list(model.features)
        if forward_hook_index is not None:
            self.register_layers(features, forward_hook_index, 'student_encoder')

        features = nn.Sequential(*features)
        return features

    def simple_decoder(self):
        return Decoder(has_bias=self._has_bias)

    def register_layers(self, model, name_list, prefix):
        for i, idx in enumerate(name_list):
            model[idx].register_forward_hook(self.get_activation(prefix+'_{}'.format(i))) # will be called every time after forward has been computed
        return model

    def get_activation(self,name):
        def hook(model, input, output):
            self.activation[name] = output
        return hook